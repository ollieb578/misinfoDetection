{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad033dc",
   "metadata": {},
   "source": [
    "This notebook constructs the knowledgebase ```.csv``` files for the fact checker to use.\n",
    "\n",
    "The constructed knowledgebases have 3 columns: statement, verdict and keywords.\n",
    "In order to construct one on a new subject, replace the ```subject``` variable and run the notebook.\n",
    "\n",
    "The stopwords list is from https://www.kaggle.com/datasets/rowhitswami/stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "import re\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = os.path.join(os.path.abspath(\"..\"), \"podcasts-transcripts\\\\training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d17c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataPath)\n",
    "print(data.columns)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['verdict'] = data['verdict'].map({0:True, 1:False})\n",
    "data = data[['statement', 'verdict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efe530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords, from https://github.com/Alir3z4/stop-words/blob/bd8cc1434faeb3449735ed570a4a392ab5d35291/english.txt\n",
    "# has been modified from this version\n",
    "\n",
    "file = open(\"english.txt\", \"r\")\n",
    "stop = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10533ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'COVID'\n",
    "\n",
    "facts = data[data['statement'].str.contains(subject)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts['keywords'] = facts['statement']\n",
    "# removes all instances of \"'s\", all punctuation, all numbers, and sets to lowercase\n",
    "facts['keywords'] = facts['keywords'].str.replace(\"’s\",'').str.replace('[^\\w\\s]','').str.replace('[\\d]','').str.lower()\n",
    "# removes stopwords\n",
    "facts['keywords'] = facts['keywords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# removes duplicates\n",
    "facts['keywords'] = facts['keywords'].str.split(\" \").map(set).str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0864bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'factbase\\\\' + subject + '.csv'\n",
    "facts.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17fb54c",
   "metadata": {},
   "source": [
    "---\n",
    "The above section works for established datasets like the Politifact one, but for the HCQ and IVM datasets a different approach is required.\n",
    "\n",
    "My current idea is to take 10,000 or so values from the dataset to use as the KB.\n",
    "If that doesn't give the results I'm looking for then my options are:\n",
    " - scrape the wikipedia page and use every sentence in the KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f301b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = os.path.join(os.path.abspath(\"..\"), \"podcasts-transcripts\\\\hcq data\\\\hcq_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af05829",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataPath)\n",
    "print(data.columns)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kb only\n",
    "data = data.rename(columns={'text': 'statement', 'pred': 'verdict'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kb only\n",
    "data.drop(data[data.verdict == 2].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for kb\n",
    "facts = data.sample(n = 5000)\n",
    "data = data.drop(facts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8418d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test set\n",
    "claims = data.sample(n = 5001)\n",
    "data = data.drop(claims.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b185e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(claims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kb only\n",
    "facts['verdict'] = facts['verdict'].map({0:True, 1:False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f1aa7",
   "metadata": {},
   "source": [
    "--- \n",
    "keyword list gen\n",
    "\n",
    "kb only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87754acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords, from https://github.com/Alir3z4/stop-words/blob/bd8cc1434faeb3449735ed570a4a392ab5d35291/english.txt\n",
    "# has been modified from this version\n",
    "\n",
    "file = open(\"english.txt\", \"r\")\n",
    "stop = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts['keywords'] = facts['statement']\n",
    "# removes all instances of \"'s\", all punctuation, all numbers, and sets to lowercase\n",
    "facts['keywords'] = facts['keywords'].str.replace(\"’s\",'').str.replace('[^\\w\\s]','').str.replace('[\\d]','').str.lower()\n",
    "# removes stopwords\n",
    "facts['keywords'] = facts['keywords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# removes duplicates\n",
    "facts['keywords'] = facts['keywords'].str.split(\" \").map(set).str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c77670",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460788f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'factbase\\\\' + \"Hydroxychloroquine_5k_3\" + '.csv'\n",
    "claims.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b5376",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_sets\\\\' + \"Hydroxychloroquine_5k_3\" + '.csv'\n",
    "claims.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c88cf",
   "metadata": {},
   "source": [
    "---\n",
    "After mixed (and mostly negative) results using the preivous approach, I've decided to take extracts from Wikipedia pages to use as a fact-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_page = wikipedia.page(\"COVID-19 misinformation\", auto_suggest=False).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2faa433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = in_page.split()\n",
    "in_page = re.sub(\"\\=.*\\=\", \"\", in_page)\n",
    "in_page = re.sub(\"\\n\", \"\", in_page)\n",
    "in_page = in_page.split(\".\")\n",
    "in_page = [sentence.strip() for sentence in in_page]\n",
    "print(in_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = pd.DataFrame({'statement':in_page})\n",
    "print (facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_page = wikipedia.page(\"List of unproven methods against COVID-19\").content\n",
    "in_page = re.sub(\"\\=.*\\=\", \"\", in_page)\n",
    "in_page = re.sub(\"\\n\", \"\", in_page)\n",
    "in_page = in_page.split(\".\")\n",
    "in_page = [sentence.strip() for sentence in in_page]\n",
    "print(in_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658faa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts2 = pd.DataFrame({'statement':in_page})\n",
    "print (facts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ea682",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = facts.append(facts2)\n",
    "facts.dropna()\n",
    "print(facts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts.drop(facts[facts['statement'].map(len) < 30].index, inplace = True)\n",
    "print(facts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts['verdict'] = True\n",
    "\n",
    "facts['keywords'] = facts['statement']\n",
    "# removes all instances of \"'s\", all punctuation, all numbers, and sets to lowercase\n",
    "facts['keywords'] = facts['keywords'].str.replace(\"’s\",'').str.replace('[^\\w\\s]','').str.replace('[\\d]','').str.lower()\n",
    "# removes stopwords\n",
    "facts['keywords'] = facts['keywords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# removes duplicates\n",
    "facts['keywords'] = facts['keywords'].str.split(\" \").map(set).str.join(\" \")\n",
    "\n",
    "print (facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'factbase\\\\' + \"fc_wiki\" + '.csv'\n",
    "facts.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa15e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
