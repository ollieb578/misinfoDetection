{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6402d4",
   "metadata": {},
   "source": [
    "Having built a semantic matcher model using Keras' guide, this notebook implements it as it would sit in the finished product. A claim and a series of facts (related to the claim) are passed through the model.\n",
    "\n",
    "A verdict is then formed based on the output of the model from the facts and the claim.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b49db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe09eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'semantic'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 128\n",
    "labels = [\"contradiction\", \"entailment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d2e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d1741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to incude the labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        # We will use base-base-uncased pretrained model.\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
    "        # encoded together and separated by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            truncation = True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        # Convert batch of encoded features to numpy array.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be7702",
   "metadata": {},
   "source": [
    "def check_similarTest(sentence1, sentence2):\n",
    "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "    )\n",
    "\n",
    "    proba = model.predict(test_data[0])[0]\n",
    "    idx = np.argmax(proba)\n",
    "    proba2 = f\"{proba[idx]: .2f}%\"\n",
    "    pred = labels[idx]\n",
    "    return pred, proba2, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ac7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(sentence1, sentence2):\n",
    "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "    )\n",
    "\n",
    "    proba = model.predict(test_data[0])[0]\n",
    "    idx = np.argmax(proba)\n",
    "    proba2 = proba[idx]\n",
    "    return idx, proba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c91133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrtwo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0.8072129)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a logical counter to sentence 2, presented as a fact\n",
    "sentence1 = \"The government does not perpetrate terrorism.\"\n",
    "# real excerpt from infowars\n",
    "sentence2 = \"All terrorism that we've looked at from the World Trade Center of Oklahoma City to Waco has been government actions.\"\n",
    "check_similarity(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd3894c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.83527046)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing direct quotation as fact\n",
    "# the claim should always relate to the originator of the quote as their name is the KB identifier\n",
    "sentence1 = \"I am orange\"\n",
    "sentence2 = \"Trump never said 'I am orange'\"\n",
    "check_similarity(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc9f10",
   "metadata": {},
   "source": [
    "# old version for lists\n",
    "\n",
    "def fact_check(claim, facts):\n",
    "    # current highest certainty\n",
    "    highest_similarity = 0\n",
    "    # the verdict decided during analysis\n",
    "    current_verdict = 0\n",
    "    \n",
    "    for fact, verdict in facts.iteritems():\n",
    "    #for fact, verdict in facts:\n",
    "        analysis = check_similarity(fact, claim)\n",
    "        \n",
    "        if analysis[1] > highest_similarity:\n",
    "            highest_similarity = analysis[1]\n",
    "\n",
    "            # inversion for non-facts in kb\n",
    "            if not verdict:\n",
    "                match analysis[0]:\n",
    "                    case 0:\n",
    "                        current_verdict = 1 \n",
    "                    case 1:\n",
    "                        current_verdict = 0\n",
    "                    case _:\n",
    "                        current_verdict = analysis[0]\n",
    "            else:\n",
    "                current_verdict = analysis[0]\n",
    "    \n",
    "    return(current_verdict, highest_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e45cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_check(claim, facts):\n",
    "    # current highest certainty\n",
    "    highest_similarity = 0\n",
    "    # the verdict decided during analysis\n",
    "    current_verdict = 0\n",
    "    \n",
    "    for row in facts.index:\n",
    "        analysis = check_similarity(facts['statement'][row], claim)\n",
    "        print(analysis)\n",
    "        \n",
    "        if analysis[1] > highest_similarity:\n",
    "            highest_similarity = analysis[1]\n",
    "        \n",
    "            # inversion for non-facts in kb\n",
    "            if not facts['verdict'][row]:\n",
    "                match analysis[0]:\n",
    "                    case 0:\n",
    "                        current_verdict = 1 \n",
    "                    case 1:\n",
    "                        current_verdict = 0\n",
    "                    case _:\n",
    "                        current_verdict = analysis[0]\n",
    "            else:\n",
    "                current_verdict = analysis[0]\n",
    "                \n",
    "            print(facts['statement'][row])\n",
    "    \n",
    "    return(current_verdict, highest_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a859c51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This is the test using a very small knowledgebase of facts about Hydroxychloroquine from the NHS website.\n",
    "The results from this test seem extremely promising.\n",
    "\n",
    "The semantic matcher is incapable of inference based on given facts, so it won't join these together, just picks the one with the highest certainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66e0dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True means the statement is true, False means it is not true.\n",
    "# these labels are used later to create the final verdict.\n",
    "\n",
    "test_kb = [[\"Hydroxychloroquine is a type of medicine called a disease-modifying anti-rheumatic drug.\", True], \n",
    "           [\"Hydroxychloroquine is used to treat inflammatory conditions like arthritis or lupus.\", True],\n",
    "           [\"Hydroxychloroquine is used to treat some skin conditions like sarcoidosis.\", True],\n",
    "           [\"Hydroxychloroquine can affect your eyes.\", True],\n",
    "           [\"Hydroxychloroquine can make you more likely to get infections.\", True],\n",
    "           [\"Hydroxychloroquine is only available on prescription.\", True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "66922b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contradiction 0.8416484\n"
     ]
    }
   ],
   "source": [
    "# here we go\n",
    "result = fact_check('Yeah that hydroxchloroquine stuff. I\\'ve heard it doesn\\'t affect your eyes.', test_kb)\n",
    "print(labels[result[0]], result[1])\n",
    "# outputs verdict and certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b48a103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contradiction 0.8643546\n"
     ]
    }
   ],
   "source": [
    "result = fact_check('I got some without a prescription.', test_kb)\n",
    "print(labels[result[0]], result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6760268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral 0.6620801\n"
     ]
    }
   ],
   "source": [
    "result = fact_check('It stops you getting infections.', test_kb)\n",
    "print(labels[result[0]], result[1])\n",
    "# not a great result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf52aca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contradiction 0.6823682\n"
     ]
    }
   ],
   "source": [
    "result = fact_check('HCQ is no good for sarcoidosis or arthritis.', test_kb)\n",
    "print(labels[result[0]], result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9f99405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contradiction 0.87413996\n"
     ]
    }
   ],
   "source": [
    "result = fact_check('They don\\'t use it for lupus, it\\'s no good for that.', test_kb)\n",
    "print(labels[result[0]], result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dfc4dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral 0.8050079\n"
     ]
    }
   ],
   "source": [
    "# obviously a joke entry, but the kb doesn't say that it's not\n",
    "result = fact_check('Hydroxychloroquine is a good soup ingredient', test_kb)\n",
    "print(labels[result[0]], result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dba14",
   "metadata": {},
   "source": [
    "---\n",
    "Below here is tests using knowledgebases built from fact-checked datasets.\n",
    "Facts are batched in as verdicts take a while to generate.\n",
    "\n",
    "Current test plan:\n",
    "    \n",
    "    Batch size = 30\n",
    "    \n",
    "    Runs if under 80% certainty = 3\n",
    "    \n",
    "Potential test plan:\n",
    "\n",
    "    If certainty under 65%\n",
    "        Run until batches exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ede2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"Obama\" + \".csv\"\n",
    "\n",
    "kbPath = os.path.join(os.path.abspath(\"\"), \"factbase\")\n",
    "kbPath = os.path.join(kbPath, subject)\n",
    "\n",
    "fact_batch_size = 30\n",
    "cycles = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a2d7fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['statement', 'verdict'], dtype='object')\n",
      "(1627, 2)\n"
     ]
    }
   ],
   "source": [
    "kb = pd.read_csv(kbPath)\n",
    "print(kb.columns)\n",
    "print(kb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a3dd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_check(claim, facts):\n",
    "    # index of last fact checked in batch\n",
    "    nextStart = 0\n",
    "    runs = 0\n",
    "    \n",
    "    best = (0, 0)\n",
    "    \n",
    "    # only proceeds with further cycles if similarity is <80%\n",
    "    while (best[1] < 0.8 and runs < cycles):\n",
    "        result = fact_check(claim, facts.iloc[nextStart:nextStart+fact_batch_size])\n",
    "        \n",
    "        if result[1] > best[1]:\n",
    "            best = result\n",
    "        \n",
    "        runs += 1\n",
    "        nextStart = nextStart + fact_batch_size\n",
    "        \n",
    "        print(\"cycle: \", runs)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5c068ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.615222)\n",
      "Says President Obamaâ€™s deal \"allows Iran to produce a nuclear weapon.\n",
      "(0, 0.9954383)\n",
      "Given how expansive our program already was,\" expanding Medicaid in New Jersey due to Obamacare \"was a relatively small expansion.\n",
      "(0, 0.96810424)\n",
      "(0, 0.9624356)\n",
      "(0, 0.9250239)\n",
      "(0, 0.9781985)\n",
      "(0, 0.942493)\n",
      "(0, 0.91812474)\n",
      "(0, 0.9724557)\n",
      "(0, 0.9913728)\n",
      "(0, 0.98982775)\n",
      "(0, 0.93472266)\n",
      "(0, 0.9712233)\n",
      "(0, 0.9856263)\n",
      "(0, 0.99031675)\n",
      "(0, 0.9834531)\n",
      "(0, 0.96479136)\n",
      "(0, 0.9908504)\n",
      "(0, 0.97857785)\n",
      "(0, 0.9720282)\n",
      "(0, 0.97691965)\n",
      "(0, 0.97931755)\n",
      "(0, 0.9809281)\n",
      "(0, 0.98718643)\n",
      "(0, 0.9936605)\n",
      "(0, 0.98643094)\n",
      "(0, 0.9850835)\n",
      "(0, 0.9905227)\n",
      "(0, 0.94265336)\n",
      "(0, 0.99163485)\n",
      "cycle:  1\n",
      "(0, 0.9954383)\n"
     ]
    }
   ],
   "source": [
    "claim = \"Obama did not allow Iran to produce nuclear weapons.\"\n",
    "\n",
    "print(batch_check(claim, kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2507f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(0+fact_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194a0ee",
   "metadata": {},
   "source": [
    "Given the output I'm getting here, it's pretty obvious the current approach doesn't work, so I decided to move to a keyword-centric solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6aef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
